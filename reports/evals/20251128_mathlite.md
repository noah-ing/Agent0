# Math-Lite Benchmark (2025-11-28 Run)

- **Command**: `caffeinate -di sh -c 'source .venv/bin/activate && python scripts/run_eval.py --suite math-lite --work-dir outputs/opencompass/progress_smoke --max-workers 1'`
- **Work dir**: `outputs/opencompass/progress_smoke/20251128_180834`
- **Executor endpoint**: `agent0_vllm` shim pointing at the live OpenAI-compatible gateway (same settings as `.env`).
- **Elapsed time**: ~20 h 32 m for inference + 15 s for evaluation aggregation (single worker).
- **Notes**:
  - SandFuzz tool-workers were kept alive throughout; `caffeinate -di` prevented macOS sleep.
  - `scripts/run_eval.py` streamed per-dataset progress while OpenCompass wrote detailed logs to `logs/infer/`.
  - GSM8K and MATH were executed back-to-back with real OpenAI API calls (no mocks).

## Results
| Dataset | Config | Metric | Mode | Score |
| --- | --- | --- | --- | --- |
| GSM8K | `gsm8k_gen_1d7fe4` | Accuracy | `gen` | **82.79** |
| MATH  | `math_0shot_gen_393424` | Accuracy | `gen` | **70.38** |

> Source files: `summary/summary_20251128_180834.(txt|csv|md)` in the same work directory.

## Follow-Ups
1. Mirror these scores into the README so new runs have a reference point.
2. Trigger downstream reporting (`scripts/generate_iteration_report.py`) once the next co-evolution loop finishes.
3. Consider running with `--max-workers 2` if rate limits permit; current single-worker throughput yields ~18 h wall-clock for MATH.
